{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb960611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 3407\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# (Optional but good practice)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Load Data ---\n",
    "data = pd.read_parquet('data/volume_pred.parquet')  # adjust path if needed\n",
    "data = data.sort_values(['ticker', 'date'])\n",
    "data = data[data['VOL'] >= 1].reset_index(drop=True)\n",
    "data['sentiment_score'] = data['sentiment_score'].fillna(0)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "data['log_vol'] = np.log1p(data['VOL'])\n",
    "\n",
    "# Encode ticker\n",
    "ticker_encoder = LabelEncoder()\n",
    "data['ticker_encoded'] = ticker_encoder.fit_transform(data['ticker'])\n",
    "\n",
    "# Define features\n",
    "input_features = [col for col in data.columns if col not in ['VOL', 'date', 'ticker', 'log_vol']]\n",
    "# Drop problematic features\n",
    "problematic_features = ['execid', 'distcd']\n",
    "input_features = [col for col in input_features if col not in problematic_features]\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "def train_test_split_time_series(df, stock_col='ticker', date_col='date', test_ratio=0.2):\n",
    "    train_dfs, test_dfs = [], []\n",
    "    for stock, group in df.groupby(stock_col):\n",
    "        group = group.sort_values(date_col)\n",
    "        split_idx = int(len(group) * (1 - test_ratio))\n",
    "        train_dfs.append(group.iloc[:split_idx])\n",
    "        test_dfs.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_dfs), pd.concat(test_dfs)\n",
    "\n",
    "train_data, test_data = train_test_split_time_series(data)\n",
    "\n",
    "# --- Scaling ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[input_features])\n",
    "train_data[input_features] = scaler.transform(train_data[input_features])\n",
    "test_data[input_features] = scaler.transform(test_data[input_features])\n",
    "\n",
    "for col in input_features:\n",
    "    if not np.isfinite(train_data[col]).all():\n",
    "        print(f\"⚠️ Feature {col} has NaN or Inf after scaling!\")\n",
    "bad_rows = ~np.isfinite(train_data[input_features]).all(axis=1)\n",
    "print(train_data.loc[bad_rows])\n",
    "\n",
    "assert np.isfinite(train_data[input_features].values).all(), \"Train data has NaN or Inf!\"\n",
    "assert np.isfinite(test_data[input_features].values).all(), \"Test data has NaN or Inf!\"\n",
    "assert np.isfinite(train_data['log_vol'].values).all(), \"Train target has NaN or InF!\"\n",
    "# --- Dataset Class ---\n",
    "class StockVolumeDataset(Dataset):\n",
    "    def __init__(self, df, input_features, target_col='log_vol', time_steps=10):\n",
    "        self.features = df[input_features].values\n",
    "        self.targets = df[target_col].values\n",
    "        self.time_steps = time_steps\n",
    "        self.X, self.y = self.build_sequences()\n",
    "\n",
    "    def build_sequences(self):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(self.features) - self.time_steps):\n",
    "            X_seq.append(self.features[i:i+self.time_steps])\n",
    "            y_seq.append(self.targets[i+self.time_steps])\n",
    "        return torch.tensor(np.array(X_seq), dtype=torch.float32), torch.tensor(np.array(y_seq), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# --- DataLoader ---\n",
    "time_steps = 10\n",
    "train_dataset = StockVolumeDataset(train_data, input_features, time_steps=time_steps)\n",
    "test_dataset = StockVolumeDataset(test_data, input_features, time_steps=time_steps)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# --- Modified Stacked Bidirectional LSTM Model ---\n",
    "class StackedBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.05):\n",
    "        super(StackedBiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the output at the last time step\n",
    "        out = lstm_out[:, -1, :]\n",
    "        hidden = torch.relu(self.fc1(out))\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "\n",
    "# --- Instantiate Improved Model ---\n",
    "model = StackedBiLSTM(input_size=len(input_features)).to(device)\n",
    "\n",
    "# --- New Optimizer with lower weight_decay ---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=5e-6)\n",
    "\n",
    "# --- Scheduler ---\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, threshold=1e-4, threshold_mode='rel')\n",
    "\n",
    "# --- Other configs ---\n",
    "criterion = nn.MSELoss()\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = np.inf\n",
    "patience = 5\n",
    "counter = 0\n",
    "num_epochs = 50\n",
    "\n",
    "# --- Start Training (With tqdm) ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "    for X_batch, y_batch in train_loader_iter:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_loader_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_loader_iter = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader_iter:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            val_preds = model(X_val)\n",
    "            val_loss = criterion(val_preds, y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_loader_iter.set_postfix(val_loss=val_loss.item())\n",
    "\n",
    "    val_loss_epoch = np.mean(val_losses)\n",
    "    scheduler.step(val_loss_epoch)\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {np.mean(train_losses):.6f} | Val Loss: {val_loss_epoch:.6f}\")\n",
    "\n",
    "    if np.isnan(val_loss_epoch):\n",
    "        print(\"⚠️ Validation loss became NaN! Stopping training.\")\n",
    "        break\n",
    "\n",
    "    if val_loss_epoch < best_loss:\n",
    "        best_loss = val_loss_epoch\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"✅ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# --- Load Best Model ---\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"Best model loaded with loss:\", best_loss)\n",
    "\n",
    "# --- Evaluation ---\n",
    "model.eval()\n",
    "pred_list, true_list = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        preds = model(X_batch).cpu().numpy()\n",
    "        true_list.append(y_batch.cpu().numpy())\n",
    "        pred_list.append(preds)\n",
    "\n",
    "pred_all_log = np.vstack(pred_list)\n",
    "true_all_log = np.vstack(true_list)\n",
    "\n",
    "# --- Inverse Transform ---\n",
    "pred_all_original = np.expm1(pred_all_log)\n",
    "true_all_original = np.expm1(true_all_log)\n",
    "\n",
    "# --- Plot in Log Space ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(true_all_log[:200], label='True log_vol')\n",
    "plt.plot(pred_all_log[:200], label='Predicted log_vol', linestyle='--')\n",
    "plt.title('Prediction vs True (First 200 samples, log space)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Evaluate on Original Scale ---\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse_orig = mean_squared_error(true_all_original, pred_all_original)\n",
    "mae_orig = mean_absolute_error(true_all_original, pred_all_original)\n",
    "r2_orig = r2_score(true_all_original, pred_all_original)\n",
    "\n",
    "print(f\"\\n--- Metrics on Original Volume Scale ---\")\n",
    "print(f\"MSE (Original Volume): {mse_orig:.4f}\")\n",
    "print(f\"MAE (Original Volume): {mae_orig:.4f}\")\n",
    "print(f\"R² (Original Volume): {r2_orig:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
